{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPgImjmQ5DI9"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.quantization import quantize_dynamic\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dgaj0FbPAtUy"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "uxsSxEypAxGx"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model size (simple function)\n",
        "def get_model_size_mb(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    # Assume 4 bytes per parameter (float32)\n",
        "    size_mb = (total_params * 4) / (1024 * 1024)\n",
        "    return size_mb\n",
        "\n",
        "original_size = get_model_size_mb(model)"
      ],
      "metadata": {
        "id": "9Yf-TfUxA2Kb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"This movie is amazing!\",\n",
        "    \"I hate this film.\",\n",
        "    \"It's okay I guess.\"\n",
        "]\n",
        "\n",
        "inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "TcWRF7s2A5Up"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw69d56DA7PN",
        "outputId": "dfed85d0-a2e1-480c-dea5-98cbf628587c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Warmup run\n",
        "with torch.no_grad():\n",
        "    _ = model(**inputs)"
      ],
      "metadata": {
        "id": "2XHHWh7fA8xD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "times = []\n",
        "for i in range(5):  # 5 runs for stability\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        original_predictions = torch.softmax(outputs.logits, dim=-1)\n",
        "    end = time.time()\n",
        "    times.append(end - start)\n",
        "\n",
        "original_time = np.mean(times)\n",
        "time_std = np.std(times)\n",
        "\n",
        "print(f\"  Avg inference time: {original_time:.4f} ¬± {time_std:.4f} seconds\")\n",
        "print(f\"  Sample prediction: {original_predictions[0].numpy()}\")\n",
        "print(f\"  Model size: {original_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaznoGO3A-bA",
        "outputId": "73ae65f5-c9ad-4e82-cd07-b08040681bb1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Avg inference time: 0.0616 ¬± 0.0073 seconds\n",
            "  Sample prediction: [1.18231525e-04 9.99881744e-01]\n",
            "  Model size: 255.41 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = quantize_dynamic(\n",
        "    model,              # Your original model\n",
        "    {nn.Linear},        # Quantize Linear layers only (safe)\n",
        "    dtype=torch.qint8   # Use INT8\n",
        ")"
      ],
      "metadata": {
        "id": "bMUZqtWxBAKu"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_size = get_model_size_mb(quantized_model)\n",
        "compression_ratio = original_size / quantized_size\n",
        "\n",
        "print(f\"  Original:   {original_size:.2f} MB\")\n",
        "print(f\"  Quantized:  {quantized_size:.2f} MB\")\n",
        "print(f\"  Compression: {compression_ratio:.2f}x smaller\")\n",
        "print(f\"  Reduction:  {((original_size - quantized_size) / original_size * 100):.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgtSB8hoBPZp",
        "outputId": "29523404-1bc4-42f2-a8d7-792126027687"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Original:   255.41 MB\n",
            "  Quantized:  91.00 MB\n",
            "  Compression: 2.81x smaller\n",
            "  Reduction:  64.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eYw_6cpBRJF",
        "outputId": "bfdbfd01-25ff-4f7d-bc7f-6ee94a585e00"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Warmup run\n",
        "with torch.no_grad():\n",
        "    _ = quantized_model(**inputs)"
      ],
      "metadata": {
        "id": "W4p5GIo6BUg5"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_times = []\n",
        "for i in range(5):\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = quantized_model(**inputs)\n",
        "        quantized_predictions = torch.softmax(outputs.logits, dim=-1)\n",
        "    end = time.time()\n",
        "    quantized_times.append(end - start)\n",
        "\n",
        "quantized_time = np.mean(quantized_times)\n",
        "quantized_std = np.std(quantized_times)\n",
        "\n",
        "print(f\"  Avg inference time: {quantized_time:.4f} ¬± {quantized_std:.4f} seconds\")\n",
        "print(f\"  Sample prediction: {quantized_predictions[0].numpy()}\")\n",
        "print(f\"  Model size: {quantized_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rbDf2L_BV6n",
        "outputId": "730180d1-ea91-47e6-fd41-9c0b88838769"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Avg inference time: 0.0335 ¬± 0.0021 seconds\n",
            "  Sample prediction: [1.3081332e-04 9.9986923e-01]\n",
            "  Model size: 91.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Comparison\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Speed comparison\n",
        "speedup = original_time / quantized_time\n",
        "print(f\"üöÄ Speed Results:\")\n",
        "print(f\"  Original time:    {original_time:.4f} seconds\")\n",
        "print(f\"  Quantized time:   {quantized_time:.4f} seconds\")\n",
        "print(f\"  Speedup:          {speedup:.2f}x faster\")\n",
        "\n",
        "# Size comparison\n",
        "print(f\"\\nüì¶ Size Results:\")\n",
        "print(f\"  Original size:    {original_size:.2f} MB\")\n",
        "print(f\"  Quantized size:   {quantized_size:.2f} MB\")\n",
        "print(f\"  Compression:      {compression_ratio:.2f}x smaller\")\n",
        "\n",
        "# Accuracy comparison\n",
        "accuracy_diff = np.mean(np.abs(original_predictions.numpy() - quantized_predictions.numpy()))\n",
        "print(f\"\\nüéØ Accuracy Results:\")\n",
        "print(f\"  Prediction difference: {accuracy_diff:.6f}\")\n",
        "\n",
        "if accuracy_diff < 0.001:\n",
        "    print(f\"  Status: ‚úÖ Excellent! Virtually no accuracy loss\")\n",
        "elif accuracy_diff < 0.01:\n",
        "    print(f\"  Status: ‚úÖ Very good! Minimal accuracy loss\")\n",
        "elif accuracy_diff < 0.05:\n",
        "    print(f\"  Status: ‚úÖ Good! Acceptable accuracy loss\")\n",
        "else:\n",
        "    print(f\"  Status: ‚ö†Ô∏è Moderate accuracy loss - check if acceptable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbA-2a3CBYiq",
        "outputId": "4c1da409-3829-4255-9c64-bb6b048fd510"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PERFORMANCE COMPARISON\n",
            "============================================================\n",
            "üöÄ Speed Results:\n",
            "  Original time:    0.0616 seconds\n",
            "  Quantized time:   0.0335 seconds\n",
            "  Speedup:          1.84x faster\n",
            "\n",
            "üì¶ Size Results:\n",
            "  Original size:    255.41 MB\n",
            "  Quantized size:   91.00 MB\n",
            "  Compression:      2.81x smaller\n",
            "\n",
            "üéØ Accuracy Results:\n",
            "  Prediction difference: 0.000035\n",
            "  Status: ‚úÖ Excellent! Virtually no accuracy loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed prediction comparison\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED PREDICTION COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "orig_preds = original_predictions.numpy()\n",
        "quant_preds = quantized_predictions.numpy()\n",
        "\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"\\nText {i+1}: '{text}'\")\n",
        "    print(f\"  Original:  [{orig_preds[i][0]:.6f}, {orig_preds[i][1]:.6f}]\")\n",
        "    print(f\"  Quantized: [{quant_preds[i][0]:.6f}, {quant_preds[i][1]:.6f}]\")\n",
        "\n",
        "    # Check if predictions match\n",
        "    orig_class = \"Positive\" if orig_preds[i][1] > 0.5 else \"Negative\"\n",
        "    quant_class = \"Positive\" if quant_preds[i][1] > 0.5 else \"Negative\"\n",
        "\n",
        "    if orig_class == quant_class:\n",
        "        print(f\"  Result: ‚úÖ {orig_class} (predictions match)\")\n",
        "    else:\n",
        "        print(f\"  Result: ‚ö†Ô∏è Mismatch! Original: {orig_class}, Quantized: {quant_class}\")\n",
        "\n",
        "    # Calculate confidence difference\n",
        "    conf_diff = abs(orig_preds[i][1] - quant_preds[i][1])\n",
        "    print(f\"  Confidence difference: {conf_diff:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUxZ5ThCBbok",
        "outputId": "4ae04cb0-b058-4ed5-8ba4-acba0b2d1fe6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DETAILED PREDICTION COMPARISON\n",
            "============================================================\n",
            "\n",
            "Text 1: 'This movie is amazing!'\n",
            "  Original:  [0.000118, 0.999882]\n",
            "  Quantized: [0.000131, 0.999869]\n",
            "  Result: ‚úÖ Positive (predictions match)\n",
            "  Confidence difference: 0.000013\n",
            "\n",
            "Text 2: 'I hate this film.'\n",
            "  Original:  [0.999687, 0.000313]\n",
            "  Quantized: [0.999680, 0.000320]\n",
            "  Result: ‚úÖ Negative (predictions match)\n",
            "  Confidence difference: 0.000007\n",
            "\n",
            "Text 3: 'It's okay I guess.'\n",
            "  Original:  [0.000229, 0.999771]\n",
            "  Quantized: [0.000314, 0.999686]\n",
            "  Result: ‚úÖ Positive (predictions match)\n",
            "  Confidence difference: 0.000085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5NurWjWBdDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}