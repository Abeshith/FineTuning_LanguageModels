# -*- coding: utf-8 -*-
"""FineTuning And Dpo Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yEiro280w782tf7kkK_PpnYSsppWIFeV
"""

pip install datasets trl bitsandbytes peft accelerate

from datasets import load_dataset, Dataset
from trl import SFTTrainer
from accelerate import Accelerator
import torch

from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, AutoPeftModelForCausalLM

from transformers import AutoModelForCausalLM , AutoTokenizer, BitsAndBytesConfig, TrainingArguments

df = load_dataset("tatsu-lab/alpaca",split ="train")

data = df.to_pandas()
data = data.sample(1000)

data['text'] = data[['instruction' ,'input','output']].apply(lambda x : "Human : " + x['instruction'] + " " + x["input"] + "\nAI:" + x['output'] ,axis = 1)

df = Dataset.from_pandas(data)

model ="tinyllama/tinyllama-1.1b-chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model,device_map="auto")

model.config.use_cache = False
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

peft_model = get_peft_model(model,lora_config)

training_args = TrainingArguments(
     output_dir="./results",
     per_device_train_batch_size=8,
     gradient_accumulation_steps=1,
     optim="paged_adamw_32bit",
     learning_rate=2e-4,
     lr_scheduler_type="cosine",
     save_strategy="epoch",
     save_total_limit=3,
     logging_steps=10,
     num_train_epochs=1,
     max_steps=100,
     fp16 = True,
 )

trainer  = SFTTrainer(
    model=peft_model,
    train_dataset=df,
    peft_config=lora_config,
    processing_class=tokenizer,
    args=training_args,
)

trainer.train()

trainer.save_model("Finetuned_Model")
tokenizer.save_pretrained("Finetuned_tokeinizer")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from datasets import load_dataset,Dataset
from trl import DPOTrainer
from peft import get_peft_model,LoraConfig
from accelerate import Accelerator

df = load_dataset("Anthropic/hh-rlhf",split="train")
df

df = df.to_pandas()

df['prompt'] = df['chosen'].apply(lambda x : x.split("Assistant: ")[0])
df['prompt'] = df['chosen'].apply(lambda x : "Assistant: " + x.split("Assistant: ")[-1])
df['rejected'] = df['rejected'].apply(lambda x : "Assistant: " + x.split("Assistant: ")[-1])
df = df.sample(1000)

data = Dataset.from_pandas(df)

pip install -U bitsandbytes

tokenizer = AutoTokenizer.from_pretrained("/content/Finetuned_tokeinizer")

model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = "/content/Finetuned_Model",
                                             device_map = Accelerator().local_process_index,
                                             torch_dtype = torch.float16)

model.config.use_cache = False
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

tokenizer.pad_token = tokenizer.eos_token

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

peft_model_ft = get_peft_model(model,lora_config)

peft_model_ft.print_trainable_parameters()

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from datasets import load_dataset,Dataset
from trl import DPOTrainer, DPOConfig

from trl import SFTConfig

dpo_config = DPOConfig(
    output_dir='./results',
    evaluation_strategy="no",
    learning_rate=5e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    save_strategy="epoch",
)

trainer = DPOTrainer(
    model=peft_model_ft,
    processing_class=tokenizer,
    args=dpo_config,
    train_dataset=data,
    peft_config=lora_config,
)

trainer.train()

trainer.save_model("finetuned-dpo")
tokenizer.save_pretrained("finetuned-tokenizer_dpo")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

original_tokenizer = AutoTokenizer.from_pretrained("tinyllama/tinyllama-1.1b-chat-v1.0")  # Original model
original_model = AutoModelForCausalLM.from_pretrained("tinyllama/tinyllama-1.1b-chat-v1.0",
                                                      torch_dtype=torch.float16,
                                                      device_map="auto")

dpo_tokenizer = AutoTokenizer.from_pretrained("finetuned-tokenizer_dpo")  # DPO model
dpo_model = AutoModelForCausalLM.from_pretrained("finetuned-dpo",
                                                torch_dtype=torch.float16,
                                                device_map="auto")

dpo_model.eval()

original_generator = pipeline(task="text-generation",
                               model=original_model,
                               tokenizer=original_tokenizer)
dpo_generator = pipeline(task="text-generation",
                          model=dpo_model,
                          tokenizer=dpo_tokenizer)

print(dpo_generator)

dpo_generator = pipeline(
    task="text-generation",
    model=dpo_model,
    tokenizer=dpo_tokenizer,
    pad_token_id=dpo_tokenizer.eos_token_id
)

if dpo_tokenizer.pad_token is None:
    dpo_tokenizer.pad_token = dpo_tokenizer.eos_token

prompt = "Translate this sentence into French: 'The sky is blue."

inputs = dpo_tokenizer(prompt, return_tensors="pt").to("cuda")
output = dpo_model.generate(
    **inputs,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.9,
    top_p=0.95,
    repetition_penalty=1.2
)

generated_text = dpo_tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

prompt = "What are some polite ways to decline an Graduation?"

inputs = dpo_tokenizer(prompt, return_tensors="pt").to("cuda")
output = dpo_model.generate(
    **inputs,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.9,
    top_p=0.95,
    repetition_penalty=1.2
)

generated_text = dpo_tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

