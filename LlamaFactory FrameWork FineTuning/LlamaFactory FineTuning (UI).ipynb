{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRPGIRwdwoEB",
        "outputId": "13f2e7e1-847a-4e89-ceb0-e5331bf78234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 26411, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 26411 (delta 97), reused 26 (delta 26), pack-reused 26224 (from 4)\u001b[K\n",
            "Receiving objects: 100% (26411/26411), 12.82 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (18921/18921), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DspLTum6wuE0",
        "outputId": "0bcd8500-d3a5-4d91-a4c1-b35f53024b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aRV-z4uwwJt",
        "outputId": "1a704ef4-5fee-41cb-86a5-c26ee8130566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMrgwAsvwxjI",
        "outputId": "f5186fb2-c517-40fa-d694-3f3465070467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJKxdYwmwy8v",
        "outputId": "f6efad7e-a196-45d2-b93d-3dfe5060e2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assets\t      docker\tLICENSE      pyproject.toml  requirements  tests\n",
            "CITATION.cff  docs\tMakefile     README.md\t     scripts\t   tests_v1\n",
            "data\t      examples\tMANIFEST.in  README_zh.md    src\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfymeTPDw0cj"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkf521MYw3MN",
        "outputId": "865f50cc-78a3-437f-aee5-98149c1b4f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adam-mini.txt\t  deepspeed.txt  galore.txt\t   minicpm-v.txt  vllm.txt\n",
            "apollo.txt\t  dev.txt\t gptq.txt\t   npu.txt\n",
            "aqlm.txt\t  eetq.txt\t hqq.txt\t   openmind.txt\n",
            "badam.txt\t  fp8-te.txt\t liger-kernel.txt  sglang.txt\n",
            "bitsandbytes.txt  fp8.txt\t metrics.txt\t   swanlab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amDJxNErxp2d"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/bitsandbytes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSNnxTq2xr2D"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/deepspeed.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vuWWpi8yE68"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/gptq.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuVT3v9byGan"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/vllm.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF8bASUkyHz_",
        "outputId": "e412ca10-ade4-450d-daa0-37bd133c6c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "Unknown command: --help.\n",
            "----------------------------------------------------------------------\n",
            "| Usage:                                                             |\n",
            "|   llamafactory-cli api -h: launch an OpenAI-style API server       |\n",
            "|   llamafactory-cli chat -h: launch a chat interface in CLI         |\n",
            "|   llamafactory-cli export -h: merge LoRA adapters and export model |\n",
            "|   llamafactory-cli train -h: train models                          |\n",
            "|   llamafactory-cli webchat -h: launch a chat interface in Web UI   |\n",
            "|   llamafactory-cli webui: launch LlamaBoard                        |\n",
            "|   llamafactory-cli env: show environment info                      |\n",
            "|   llamafactory-cli version: show version info                      |\n",
            "| Hint: You can use `lmf` as a shortcut for `llamafactory-cli`.      |\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!python -m llamafactory.cli --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS678vV4yTxl",
        "outputId": "9c33b3e9-9339-4c43-f37a-e37c1a4d4a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1ZWkEd2zLwI",
        "outputId": "97f53c28-6785-40a0-d8f7-fbf3ef5520c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXeEycOezNLS",
        "outputId": "6b609594-a64f-484f-86bf-7258eecb50fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "api.py\tllamafactory  train.py\twebui.py\n"
          ]
        }
      ],
      "source": [
        "!ls /content/LLaMA-Factory/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO8Fc6cbzP75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GRADIO_SHARE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2fl9CPa9eAS"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq5c-j_67boB",
        "outputId": "f3eb60fa-0941-4d13-9699-16ff5115dbdb"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login, whoami\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIVurAUP8faq",
        "outputId": "f8e04963-0fed-4d96-e6c4-0ad55d028cd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'user',\n",
              " 'id': '67627776b654028fc32c7f02',\n",
              " 'name': 'Abeshith',\n",
              " 'fullname': 'Abeshith Kumaravel',\n",
              " 'email': 'abheshith7@gmail.com',\n",
              " 'emailVerified': True,\n",
              " 'canPay': False,\n",
              " 'billingMode': 'prepaid',\n",
              " 'periodEnd': 1772323200,\n",
              " 'isPro': False,\n",
              " 'avatarUrl': '/avatars/7a637e4db3138d57bde86e7428215360.svg',\n",
              " 'orgs': [],\n",
              " 'auth': {'type': 'access_token',\n",
              "  'accessToken': {'displayName': 'Finetuning_Demo',\n",
              "   'role': 'read',\n",
              "   'createdAt': '2025-11-07T09:59:50.778Z'}}}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "whoami()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J666vilUzS6g",
        "outputId": "8111c2c7-3623-449d-84e7-09c6e0ce4209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n",
            "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://3fac7f6b2b4b424e90.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "[INFO|2026-02-13 08:17:26] llamafactory.hparams.parser:459 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|2026-02-13 08:17:34] llamafactory.data.template:144 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|2026-02-13 08:17:34] llamafactory.data.loader:144 >> Loading dataset harpomaxx/unix-commands...\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 2045, 708, 1490, 476, 102599, 14837, 17145, 235265, 1646, 2027, 578, 8702, 7880, 685, 476, 102599, 17145, 235265, 108, 4851, 235348, 20156, 27744, 6274, 235345, 80836, 110, 107, 108, 106, 2516, 108, 235283, 6274, 235283, 4851, 235248, 108, 4851, 235348, 20156, 27744, 6274, 235345, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "You are now a Unix OS terminal. You act and respond exactly as a Unix terminal.\n",
            "root@localhost:/home# pwd\n",
            "\n",
            "\n",
            "<end_of_turn>\n",
            "<start_of_turn>model\n",
            "/home/root \n",
            "root@localhost:/home#<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235283, 6274, 235283, 4851, 235248, 108, 4851, 235348, 20156, 27744, 6274, 235345, 107, 108]\n",
            "labels:\n",
            "/home/root \n",
            "root@localhost:/home#<end_of_turn>\n",
            "\n",
            "[INFO|2026-02-13 08:19:36] llamafactory.model.model_utils.kv_cache:144 >> KV cache is disabled during training.\n",
            "[INFO|2026-02-13 08:20:43] llamafactory.model.model_utils.checkpointing:144 >> Gradient checkpointing enabled.\n",
            "[INFO|2026-02-13 08:20:43] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2026-02-13 08:20:43] llamafactory.model.adapter:144 >> Upcasting trainable params to float32.\n",
            "[INFO|2026-02-13 08:20:43] llamafactory.model.adapter:144 >> Fine-tuning method: LoRA\n",
            "[INFO|2026-02-13 08:20:43] llamafactory.model.model_utils.misc:144 >> Found linear modules: gate_proj,q_proj,down_proj,v_proj,up_proj,o_proj,k_proj\n",
            "\n",
            "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
            "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires Python < 3.14 and Python GIL enabled and Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_ALLOC_CONF='expandable_segments:True,max_split_size_mb:256,garbage_collection_threshold:0.7' for memory saving.\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
            "\u001b[36mDEBUG\u001b[0m BitBLAS import failed: No module named 'bitblas'                         \n",
            "\u001b[36mDEBUG\u001b[0m Skipping qlinear module import `bitblas_target_detector`: No module named 'thefuzz'\n",
            "\u001b[32mINFO\u001b[0m  \n",
            "\n",
            "_____/\\\\\\\\\\\\\\\\\\\\\\\\__/\\\\\\\\\\\\\\\\\\\\\\\\\\____/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\______________________/\\\\\\________/\\\\\\\\____________/\\\\\\\\_______________________/\\\\\\__________________/\\\\\\\\\\\\____\n",
            " ___/\\\\\\//////////__\\/\\\\\\/////////\\\\\\_\\///////\\\\\\/////____________________/\\\\\\\\/\\\\\\\\____\\/\\\\\\\\\\\\________/\\\\\\\\\\\\______________________\\/\\\\\\_________________\\////\\\\\\____\n",
            "  __/\\\\\\_____________\\/\\\\\\_______\\/\\\\\\_______\\/\\\\\\_______________________/\\\\\\//\\////\\\\\\__\\/\\\\\\//\\\\\\____/\\\\\\//\\\\\\______________________\\/\\\\\\____________________\\/\\\\\\____\n",
            "   _\\/\\\\\\____/\\\\\\\\\\\\\\_\\/\\\\\\\\\\\\\\\\\\\\\\\\\\/________\\/\\\\\\________/\\\\\\\\\\\\\\\\\\\\\\__/\\\\\\______\\//\\\\\\_\\/\\\\\\\\///\\\\\\/\\\\\\/_\\/\\\\\\_____/\\\\\\\\\\___________\\/\\\\\\______/\\\\\\\\\\\\\\\\_____\\/\\\\\\____\n",
            "    _\\/\\\\\\___\\/////\\\\\\_\\/\\\\\\/////////__________\\/\\\\\\_______\\///////////__\\//\\\\\\______/\\\\\\__\\/\\\\\\__\\///\\\\\\/___\\/\\\\\\___/\\\\\\///\\\\\\____/\\\\\\\\\\\\\\\\\\____/\\\\\\/////\\\\\\____\\/\\\\\\____\n",
            "     _\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\___________________\\/\\\\\\______________________\\///\\\\\\\\/\\\\\\\\/___\\/\\\\\\____\\///_____\\/\\\\\\__/\\\\\\__\\//\\\\\\__/\\\\\\////\\\\\\___/\\\\\\\\\\\\\\\\\\\\\\_____\\/\\\\\\____\n",
            "      _\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\___________________\\/\\\\\\________________________\\////\\\\\\//_____\\/\\\\\\_____________\\/\\\\\\_\\//\\\\\\__/\\\\\\__\\/\\\\\\__\\/\\\\\\__\\//\\\\///////______\\/\\\\\\____\n",
            "       _\\//\\\\\\\\\\\\\\\\\\\\\\\\/__\\/\\\\\\___________________\\/\\\\\\___________________________\\///\\\\\\\\\\\\__\\/\\\\\\_____________\\/\\\\\\__\\///\\\\\\\\\\/___\\//\\\\\\\\\\\\\\/\\\\__\\//\\\\\\\\\\\\\\\\\\\\__/\\\\\\\\\\\\\\\\\\_\n",
            "        __\\////////////____\\///____________________\\///______________________________\\//////___\\///______________\\///_____\\/////______\\///////\\//____\\//////////__\\/////////__\n",
            "\n",
            "[INFO|2026-02-13 08:20:50] llamafactory.model.loader:144 >> trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|2026-02-13 08:21:12] llamafactory.train.callbacks:144 >> {'loss': 2.6594, 'learning_rate': 4.5656e-05, 'epoch': 0.80, 'throughput': 649.29}\n",
            "{'loss': '2.659', 'grad_norm': '3.544', 'learning_rate': '4.566e-05', 'epoch': '0.8', 'num_input_tokens_seen': 12416, 'train_runtime': '19.13', 'train_tokens_per_second': '649.2'}\n",
            "[INFO|2026-02-13 08:21:28] llamafactory.train.callbacks:144 >> {'loss': 2.1974, 'learning_rate': 3.0563e-05, 'epoch': 1.48, 'throughput': 670.76}\n",
            "{'loss': '2.197', 'grad_norm': '2.549', 'learning_rate': '3.056e-05', 'epoch': '1.48', 'num_input_tokens_seen': 23168, 'train_runtime': '34.54', 'train_tokens_per_second': '670.7'}\n",
            "[INFO|2026-02-13 08:21:43] llamafactory.train.callbacks:144 >> {'loss': 1.8851, 'learning_rate': 1.2500e-05, 'epoch': 2.16, 'throughput': 677.41}\n",
            "{'loss': '1.885', 'grad_norm': '2.804', 'learning_rate': '1.25e-05', 'epoch': '2.16', 'num_input_tokens_seen': 33968, 'train_runtime': '50.15', 'train_tokens_per_second': '677.4'}\n",
            "[INFO|2026-02-13 08:22:02] llamafactory.train.callbacks:144 >> {'loss': 1.7562, 'learning_rate': 1.1107e-06, 'epoch': 2.96, 'throughput': 681.74}\n",
            "{'loss': '1.756', 'grad_norm': '2.923', 'learning_rate': '1.111e-06', 'epoch': '2.96', 'num_input_tokens_seen': 47200, 'train_runtime': '69.24', 'train_tokens_per_second': '681.7'}\n",
            "{'train_runtime': '71.24', 'train_samples_per_second': '4.211', 'train_steps_per_second': '0.295', 'train_loss': '2.11', 'epoch': '3', 'num_input_tokens_seen': 47856}\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  num_input_tokens_seen    =      47856\n",
            "  total_flos               =   532610GF\n",
            "  train_loss               =     2.1099\n",
            "  train_runtime            = 0:01:11.23\n",
            "  train_samples_per_second =      4.211\n",
            "  train_steps_per_second   =      0.295\n",
            "Figure saved at: saves/Gemma-1.1-2B-Instruct/lora/train_2026-02-13-08-13-39/training_loss.png\n",
            "[WARNING|2026-02-13 08:22:05] llamafactory.extras.ploting:149 >> No metric eval_loss to plot.\n",
            "[WARNING|2026-02-13 08:22:05] llamafactory.extras.ploting:149 >> No metric eval_accuracy to plot.\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:31:28,987 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:31:28,991 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:31:32,347 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:31:32,348 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:31:32,457 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:31:32,458 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2026-02-13 08:31:35] llamafactory.data.template:144 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:31:35,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:31:35,404 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:327] 2026-02-13 08:31:35,404 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|2026-02-13 08:31:35] llamafactory.model.model_utils.kv_cache:144 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:732] 2026-02-13 08:31:35,894 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:801] 2026-02-13 08:31:35,895 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
            "[INFO|configuration_utils.py:1014] 2026-02-13 08:31:35,896 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"use_cache\": true\n",
            "}\n",
            "\n",
            "[INFO|accelerate.py:214] 2026-02-13 08:31:35,951 >> We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Loading weights: 100% 164/164 [00:12<00:00, 13.07it/s, Materializing param=model.norm.weight]\n",
            "[INFO|configuration_utils.py:967] 2026-02-13 08:31:48,896 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/generation_config.json\n",
            "[INFO|configuration_utils.py:1014] 2026-02-13 08:31:48,897 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|dynamic_module_utils.py:406] 2026-02-13 08:31:48,992 >> Could not locate the custom_generate/generate.py inside google/gemma-1.1-2b-it.\n",
            "[INFO|2026-02-13 08:31:48] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2026-02-13 08:31:48] llamafactory.model.loader:144 >> all params: 2,506,172,416\n",
            "[WARNING|2026-02-13 08:31:48] llamafactory.chat.hf_engine:155 >> There is no current event loop, creating a new one.\n",
            "Keyboard interruption in main thread... closing server.Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3043, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LLaMA-Factory/src/webui.py\", line 31, in <module>\n",
            "    main()\n",
            "  File \"/content/LLaMA-Factory/src/webui.py\", line 27, in main\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2950, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3045, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/console_capture.py\", line 176, in write_with_callbacks\n",
            "    stack.enter_context(_reset_on_exception())\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 527, in enter_context\n",
            "    self._push_cm_exit(cm, _exit)\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 545, in _push_cm_exit\n",
            "    _exit_wrapper = self._create_exit_wrapper(cm, cm_exit)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 471, in _create_exit_wrapper\n",
            "    @staticmethod\n",
            "\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://3fac7f6b2b4b424e90.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/LLaMA-Factory/src/webui.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04378942cd674a32ba3e7cbe714b9dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a3358b913764125b34ebf8ba915e2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04378942cd674a32ba3e7cbe714b9dfe",
            "max": 164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_998c660ed43547edbf9205afef2a8cc3",
            "value": 164
          }
        },
        "449fcead96fe4cd2996cd93e2e3b5304": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532ff4577ea0406e94f4b25bd0674156": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dcfdcaf84db4ae0b3477ca64ea5dfe4",
              "IPY_MODEL_1a3358b913764125b34ebf8ba915e2bc",
              "IPY_MODEL_c6b03e5ab74a4448848f60e373fe48b2"
            ],
            "layout": "IPY_MODEL_baa4c30402fc4305b6a86c31bd03791b"
          }
        },
        "586fbe194c054a2bb361b681fa5e1d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dcfdcaf84db4ae0b3477ca64ea5dfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586fbe194c054a2bb361b681fa5e1d6c",
            "placeholder": "​",
            "style": "IPY_MODEL_449fcead96fe4cd2996cd93e2e3b5304",
            "value": "Loading weights: 100%"
          }
        },
        "998c660ed43547edbf9205afef2a8cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baa4c30402fc4305b6a86c31bd03791b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bea2195f5634414baa797f5d84e06ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b03e5ab74a4448848f60e373fe48b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bea2195f5634414baa797f5d84e06ba4",
            "placeholder": "​",
            "style": "IPY_MODEL_de2bacd061a3406095d20e5f4bd407b9",
            "value": " 164/164 [00:12&lt;00:00, 11.72it/s, Materializing param=model.norm.weight]"
          }
        },
        "de2bacd061a3406095d20e5f4bd407b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
