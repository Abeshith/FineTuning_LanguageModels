{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRPGIRwdwoEB",
        "outputId": "13f2e7e1-847a-4e89-ceb0-e5331bf78234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 26411, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 26411 (delta 97), reused 26 (delta 26), pack-reused 26224 (from 4)\u001b[K\n",
            "Receiving objects: 100% (26411/26411), 12.82 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (18921/18921), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DspLTum6wuE0",
        "outputId": "0bcd8500-d3a5-4d91-a4c1-b35f53024b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aRV-z4uwwJt",
        "outputId": "1a704ef4-5fee-41cb-86a5-c26ee8130566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMrgwAsvwxjI",
        "outputId": "f5186fb2-c517-40fa-d694-3f3465070467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJKxdYwmwy8v",
        "outputId": "f6efad7e-a196-45d2-b93d-3dfe5060e2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assets\t      docker\tLICENSE      pyproject.toml  requirements  tests\n",
            "CITATION.cff  docs\tMakefile     README.md\t     scripts\t   tests_v1\n",
            "data\t      examples\tMANIFEST.in  README_zh.md    src\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfymeTPDw0cj"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkf521MYw3MN",
        "outputId": "865f50cc-78a3-437f-aee5-98149c1b4f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adam-mini.txt\t  deepspeed.txt  galore.txt\t   minicpm-v.txt  vllm.txt\n",
            "apollo.txt\t  dev.txt\t gptq.txt\t   npu.txt\n",
            "aqlm.txt\t  eetq.txt\t hqq.txt\t   openmind.txt\n",
            "badam.txt\t  fp8-te.txt\t liger-kernel.txt  sglang.txt\n",
            "bitsandbytes.txt  fp8.txt\t metrics.txt\t   swanlab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amDJxNErxp2d"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/bitsandbytes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSNnxTq2xr2D"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/deepspeed.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vuWWpi8yE68"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/gptq.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuVT3v9byGan"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements/vllm.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF8bASUkyHz_",
        "outputId": "e412ca10-ade4-450d-daa0-37bd133c6c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "Unknown command: --help.\n",
            "----------------------------------------------------------------------\n",
            "| Usage:                                                             |\n",
            "|   llamafactory-cli api -h: launch an OpenAI-style API server       |\n",
            "|   llamafactory-cli chat -h: launch a chat interface in CLI         |\n",
            "|   llamafactory-cli export -h: merge LoRA adapters and export model |\n",
            "|   llamafactory-cli train -h: train models                          |\n",
            "|   llamafactory-cli webchat -h: launch a chat interface in Web UI   |\n",
            "|   llamafactory-cli webui: launch LlamaBoard                        |\n",
            "|   llamafactory-cli env: show environment info                      |\n",
            "|   llamafactory-cli version: show version info                      |\n",
            "| Hint: You can use `lmf` as a shortcut for `llamafactory-cli`.      |\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!python -m llamafactory.cli --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS678vV4yTxl",
        "outputId": "9c33b3e9-9339-4c43-f37a-e37c1a4d4a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1ZWkEd2zLwI",
        "outputId": "97f53c28-6785-40a0-d8f7-fbf3ef5520c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXeEycOezNLS",
        "outputId": "6b609594-a64f-484f-86bf-7258eecb50fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "api.py\tllamafactory  train.py\twebui.py\n"
          ]
        }
      ],
      "source": [
        "!ls /content/LLaMA-Factory/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO8Fc6cbzP75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GRADIO_SHARE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2fl9CPa9eAS"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq5c-j_67boB",
        "outputId": "f3eb60fa-0941-4d13-9699-16ff5115dbdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub) (8.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login, whoami\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIVurAUP8faq",
        "outputId": "f8e04963-0fed-4d96-e6c4-0ad55d028cd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'user',\n",
              " 'id': '67627776b654028fc32c7f02',\n",
              " 'name': 'Abeshith',\n",
              " 'fullname': 'Abeshith Kumaravel',\n",
              " 'email': 'abheshith7@gmail.com',\n",
              " 'emailVerified': True,\n",
              " 'canPay': False,\n",
              " 'billingMode': 'prepaid',\n",
              " 'periodEnd': 1772323200,\n",
              " 'isPro': False,\n",
              " 'avatarUrl': '/avatars/7a637e4db3138d57bde86e7428215360.svg',\n",
              " 'orgs': [],\n",
              " 'auth': {'type': 'access_token',\n",
              "  'accessToken': {'displayName': 'Finetuning_Demo',\n",
              "   'role': 'read',\n",
              "   'createdAt': '2025-11-07T09:59:50.778Z'}}}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "whoami()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ItecGzwA8y91",
        "outputId": "ed9d962d-01c6-4702-fb74-37b8056ff421"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_hub_download(\n",
        "    repo_id=\"google/gemma-1.1-2b-it\",\n",
        "    filename=\"config.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFVWHwewzZfd",
        "outputId": "b6c05686-7321-4ecf-afa9-d93e4de1d9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXBhzt3YJqel",
        "outputId": "bf141f53-89fe-4225-f0bd-cb8c60709bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwk7YGyBJsBq",
        "outputId": "05e5acb2-b255-4be9-aaed-d80366002d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb 13 08:42:59 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crnwEG7AJt07",
        "outputId": "2b52c27e-9050-48bd-c05b-32e98a5e0f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J59NBWO_JvW5",
        "outputId": "3b8423c6-be08-44b2-8d67-19feaec1283b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "[WARNING|2026-02-13 08:43:16] llamafactory.hparams.parser:149 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2026-02-13 08:43:16] llamafactory.hparams.parser:459 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:43:16,416 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:43:16,419 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:43:19,537 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:43:19,538 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:43:19,628 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:43:19,629 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2026-02-13 08:43:22] llamafactory.data.template:144 >> Replace eos token: <end_of_turn>.\n",
            "[INFO|2026-02-13 08:43:22] llamafactory.data.loader:144 >> Loading dataset alpaca_en_demo.json...\n",
            "Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 999 examples [00:00, 49109.37 examples/s]\n",
            "Converting format of dataset (num_proc=4): 100% 999/999 [00:00<00:00, 2383.94 examples/s]\n",
            "Running tokenizer on dataset (num_proc=4): 100% 899/899 [00:27<00:00, 32.78 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 1841, 12382, 798, 692, 5246, 1185, 24640, 476, 1497, 235290, 961, 5078, 235336, 107, 108, 106, 2516, 108, 4115, 24640, 476, 1497, 235290, 961, 5078, 235269, 692, 798, 5246, 577, 3142, 3757, 12382, 235269, 3359, 235292, 109, 235274, 235265, 125857, 31724, 235292, 3279, 476, 5543, 6431, 576, 3381, 235269, 665, 798, 614, 5988, 577, 7433, 674, 573, 14533, 235269, 3411, 235269, 578, 7061, 5822, 14097, 6900, 573, 4805, 5078, 235265, 714, 10476, 4026, 577, 2523, 3387, 6137, 577, 573, 4691, 578, 1501, 2821, 674, 4553, 22087, 43494, 774, 7489, 577, 1580, 235265, 109, 235284, 235265, 61778, 573, 11554, 15504, 235292, 6002, 235290, 961, 3381, 798, 6645, 3831, 72800, 689, 34472, 604, 573, 11554, 235265, 1165, 235303, 235256, 2845, 577, 2745, 573, 3381, 7103, 235269, 46560, 578, 3980, 577, 1682, 235269, 17704, 908, 1497, 12487, 576, 2793, 675, 1334, 167542, 235269, 5191, 235269, 578, 1156, 9095, 6635, 235265, 109, 235304, 235265, 21427, 235290, 73481, 235292, 3279, 476, 5543, 6431, 576, 3381, 235269, 1104, 708, 978, 10353, 604, 63914, 10266, 577, 19578, 1593, 235265, 1165, 235303, 235256, 2845, 604, 573, 10476, 577, 21662, 2252, 235290, 3534, 573, 5078, 578, 7433, 674, 832, 2113, 7659, 603, 13650, 578, 908, 235290, 511, 235290, 1545, 235265, 109, 235310, 235265, 19847, 235292, 586, 1497, 235290, 961, 5078, 2004, 614, 1578, 235290, 63173, 235269, 675, 476, 3110, 5449, 578, 24742, 3781, 576, 2113, 235265, 714, 10476, 1249, 1476, 577, 87474, 12487, 235269, 1843, 76655, 235269, 689, 3104, 671, 24430, 577, 1707, 5608, 573, 11554, 1593, 573, 3381, 235265, 109, 235308, 235265, 4897, 4815, 235292, 65402, 476, 1497, 235290, 961, 5078, 798, 614, 476, 1069, 235290, 81839, 2185, 235269, 25291, 6733, 28243, 576, 71648, 578, 15136, 6137, 577, 8637, 235265, 1165, 235303, 235256, 2845, 604, 573, 10476, 577, 12607, 1024, 1069, 16347, 235269, 7708, 22527, 9082, 578, 17704, 573, 6911, 1280, 98125, 59815, 235265, 109, 23081, 235269, 24640, 476, 1497, 235290, 961, 5078, 798, 614, 476, 21854, 901, 55524, 2185, 235269, 25291, 476, 30942, 10476, 577, 7433, 674, 573, 2048, 6431, 603, 1578, 235290, 25513, 235269, 13650, 578, 30509, 604, 573, 11554, 235265, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "What challenges can you expect when editing a long-form article?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "When editing a long-form article, you can expect to face several challenges, including:\n",
            "\n",
            "1. Maintaining consistency: With a longer piece of content, it can be difficult to ensure that the tone, style, and voice remain consistent throughout the entire article. The editor needs to pay close attention to the details and make sure that everything flows smoothly from beginning to end.\n",
            "\n",
            "2. Keeping the reader engaged: Long-form content can easily become tedious or overwhelming for the reader. It's important to keep the content interesting, informative and easy to read, breaking up long sections of text with subheadings, images, and other visual elements.\n",
            "\n",
            "3. Fact-checking: With a longer piece of content, there are more opportunities for factual errors to slip through. It's important for the editor to thoroughly fact-check the article and ensure that all information presented is accurate and up-to-date.\n",
            "\n",
            "4. Organization: A long-form article must be well-organized, with a clear structure and logical flow of information. The editor may need to rearrange sections, add headings, or create an outline to help guide the reader through the content.\n",
            "\n",
            "5. Time management: Editing a long-form article can be a time-consuming process, requiring multiple rounds of revisions and careful attention to detail. It's important for the editor to manage their time effectively, setting realistic goals and breaking the task into manageable chunks.\n",
            "\n",
            "Overall, editing a long-form article can be a challenging but rewarding process, requiring a skilled editor to ensure that the final piece is well-written, accurate and engaging for the reader.<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4115, 24640, 476, 1497, 235290, 961, 5078, 235269, 692, 798, 5246, 577, 3142, 3757, 12382, 235269, 3359, 235292, 109, 235274, 235265, 125857, 31724, 235292, 3279, 476, 5543, 6431, 576, 3381, 235269, 665, 798, 614, 5988, 577, 7433, 674, 573, 14533, 235269, 3411, 235269, 578, 7061, 5822, 14097, 6900, 573, 4805, 5078, 235265, 714, 10476, 4026, 577, 2523, 3387, 6137, 577, 573, 4691, 578, 1501, 2821, 674, 4553, 22087, 43494, 774, 7489, 577, 1580, 235265, 109, 235284, 235265, 61778, 573, 11554, 15504, 235292, 6002, 235290, 961, 3381, 798, 6645, 3831, 72800, 689, 34472, 604, 573, 11554, 235265, 1165, 235303, 235256, 2845, 577, 2745, 573, 3381, 7103, 235269, 46560, 578, 3980, 577, 1682, 235269, 17704, 908, 1497, 12487, 576, 2793, 675, 1334, 167542, 235269, 5191, 235269, 578, 1156, 9095, 6635, 235265, 109, 235304, 235265, 21427, 235290, 73481, 235292, 3279, 476, 5543, 6431, 576, 3381, 235269, 1104, 708, 978, 10353, 604, 63914, 10266, 577, 19578, 1593, 235265, 1165, 235303, 235256, 2845, 604, 573, 10476, 577, 21662, 2252, 235290, 3534, 573, 5078, 578, 7433, 674, 832, 2113, 7659, 603, 13650, 578, 908, 235290, 511, 235290, 1545, 235265, 109, 235310, 235265, 19847, 235292, 586, 1497, 235290, 961, 5078, 2004, 614, 1578, 235290, 63173, 235269, 675, 476, 3110, 5449, 578, 24742, 3781, 576, 2113, 235265, 714, 10476, 1249, 1476, 577, 87474, 12487, 235269, 1843, 76655, 235269, 689, 3104, 671, 24430, 577, 1707, 5608, 573, 11554, 1593, 573, 3381, 235265, 109, 235308, 235265, 4897, 4815, 235292, 65402, 476, 1497, 235290, 961, 5078, 798, 614, 476, 1069, 235290, 81839, 2185, 235269, 25291, 6733, 28243, 576, 71648, 578, 15136, 6137, 577, 8637, 235265, 1165, 235303, 235256, 2845, 604, 573, 10476, 577, 12607, 1024, 1069, 16347, 235269, 7708, 22527, 9082, 578, 17704, 573, 6911, 1280, 98125, 59815, 235265, 109, 23081, 235269, 24640, 476, 1497, 235290, 961, 5078, 798, 614, 476, 21854, 901, 55524, 2185, 235269, 25291, 476, 30942, 10476, 577, 7433, 674, 573, 2048, 6431, 603, 1578, 235290, 25513, 235269, 13650, 578, 30509, 604, 573, 11554, 235265, 107, 108]\n",
            "labels:\n",
            "When editing a long-form article, you can expect to face several challenges, including:\n",
            "\n",
            "1. Maintaining consistency: With a longer piece of content, it can be difficult to ensure that the tone, style, and voice remain consistent throughout the entire article. The editor needs to pay close attention to the details and make sure that everything flows smoothly from beginning to end.\n",
            "\n",
            "2. Keeping the reader engaged: Long-form content can easily become tedious or overwhelming for the reader. It's important to keep the content interesting, informative and easy to read, breaking up long sections of text with subheadings, images, and other visual elements.\n",
            "\n",
            "3. Fact-checking: With a longer piece of content, there are more opportunities for factual errors to slip through. It's important for the editor to thoroughly fact-check the article and ensure that all information presented is accurate and up-to-date.\n",
            "\n",
            "4. Organization: A long-form article must be well-organized, with a clear structure and logical flow of information. The editor may need to rearrange sections, add headings, or create an outline to help guide the reader through the content.\n",
            "\n",
            "5. Time management: Editing a long-form article can be a time-consuming process, requiring multiple rounds of revisions and careful attention to detail. It's important for the editor to manage their time effectively, setting realistic goals and breaking the task into manageable chunks.\n",
            "\n",
            "Overall, editing a long-form article can be a challenging but rewarding process, requiring a skilled editor to ensure that the final piece is well-written, accurate and engaging for the reader.<end_of_turn>\n",
            "\n",
            "Running tokenizer on dataset (num_proc=4): 100% 100/100 [00:26<00:00,  3.72 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[2, 106, 1645, 108, 24985, 573, 2412, 3352, 235269, 4152, 921, 7725, 1618, 575, 60992, 2184, 235265, 235248, 109, 235309, 235310, 235269, 235248, 235321, 235269, 235248, 235276, 235269, 235248, 235315, 235269, 235248, 235274, 235308, 235307, 108, 235309, 235310, 235269, 235248, 235321, 235269, 235248, 235276, 235269, 235248, 235315, 235269, 235248, 235274, 235308, 235307, 107, 108, 106, 2516, 108, 235274, 235308, 108, 235315, 108, 235321, 108, 235310, 108, 235276, 107, 108]\n",
            "inputs:\n",
            "<bos><start_of_turn>user\n",
            "Given the following array, print out maximum value in descending order. \n",
            "\n",
            "[4, 8, 0, 9, 15]\n",
            "[4, 8, 0, 9, 15]<end_of_turn>\n",
            "<start_of_turn>model\n",
            "15\n",
            "9\n",
            "8\n",
            "4\n",
            "0<end_of_turn>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235274, 235308, 108, 235315, 108, 235321, 108, 235310, 108, 235276, 107, 108]\n",
            "labels:\n",
            "15\n",
            "9\n",
            "8\n",
            "4\n",
            "0<end_of_turn>\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:44:28,098 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:44:28,099 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|2026-02-13 08:44:28] llamafactory.model.model_utils.quantization:144 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2026-02-13 08:44:28] llamafactory.model.model_utils.kv_cache:144 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:732] 2026-02-13 08:44:33,511 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:801] 2026-02-13 08:44:33,512 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
            "[INFO|configuration_utils.py:1014] 2026-02-13 08:44:33,513 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading weights: 100% 164/164 [00:05<00:00, 27.94it/s, Materializing param=model.norm.weight]\n",
            "[INFO|configuration_utils.py:967] 2026-02-13 08:44:39,884 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/generation_config.json\n",
            "[INFO|configuration_utils.py:1014] 2026-02-13 08:44:39,885 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|2026-02-13 08:44:39] llamafactory.model.model_utils.checkpointing:144 >> Gradient checkpointing enabled.\n",
            "[INFO|2026-02-13 08:44:39] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2026-02-13 08:44:39] llamafactory.model.adapter:144 >> Upcasting trainable params to float32.\n",
            "[INFO|2026-02-13 08:44:39] llamafactory.model.adapter:144 >> Fine-tuning method: LoRA\n",
            "[INFO|2026-02-13 08:44:39] llamafactory.model.model_utils.misc:144 >> Found linear modules: q_proj,k_proj,down_proj,v_proj,up_proj,gate_proj,o_proj\n",
            "[INFO|2026-02-13 08:44:40] llamafactory.model.loader:144 >> trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[WARNING|trainer.py:922] 2026-02-13 08:44:41,297 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 107}.\n",
            "[INFO|trainer.py:2383] 2026-02-13 08:44:41,789 >> ***** Running training *****\n",
            "[INFO|trainer.py:2384] 2026-02-13 08:44:41,789 >>   Num examples = 899\n",
            "[INFO|trainer.py:2385] 2026-02-13 08:44:41,789 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2386] 2026-02-13 08:44:41,789 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2389] 2026-02-13 08:44:41,789 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2390] 2026-02-13 08:44:41,789 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2391] 2026-02-13 08:44:41,789 >>   Total optimization steps = 225\n",
            "[INFO|trainer.py:2392] 2026-02-13 08:44:41,791 >>   Number of trainable parameters = 9,805,824\n",
            "{'loss': '2.515', 'grad_norm': '5.959', 'learning_rate': '9.998e-05', 'epoch': '0.04449'}\n",
            "{'loss': '1.747', 'grad_norm': '2.08', 'learning_rate': '9.925e-05', 'epoch': '0.08899'}\n",
            "{'loss': '1.52', 'grad_norm': '1.857', 'learning_rate': '9.751e-05', 'epoch': '0.1335'}\n",
            "{'loss': '1.436', 'grad_norm': '3.763', 'learning_rate': '9.478e-05', 'epoch': '0.178'}\n",
            "{'loss': '1.478', 'grad_norm': '2.334', 'learning_rate': '9.112e-05', 'epoch': '0.2225'}\n",
            "{'loss': '1.3', 'grad_norm': '1.919', 'learning_rate': '8.661e-05', 'epoch': '0.267'}\n",
            "{'loss': '1.39', 'grad_norm': '1.691', 'learning_rate': '8.134e-05', 'epoch': '0.3115'}\n",
            "{'loss': '1.409', 'grad_norm': '1.406', 'learning_rate': '7.541e-05', 'epoch': '0.356'}\n",
            "{'loss': '1.197', 'grad_norm': '2.837', 'learning_rate': '6.897e-05', 'epoch': '0.4004'}\n",
            "{'loss': '1.323', 'grad_norm': '1.795', 'learning_rate': '6.213e-05', 'epoch': '0.4449'}\n",
            "{'loss': '1.322', 'grad_norm': '2.413', 'learning_rate': '5.504e-05', 'epoch': '0.4894'}\n",
            "{'loss': '1.248', 'grad_norm': '1.782', 'learning_rate': '4.784e-05', 'epoch': '0.5339'}\n",
            "{'loss': '1.403', 'grad_norm': '1.442', 'learning_rate': '4.069e-05', 'epoch': '0.5784'}\n",
            "{'loss': '1.164', 'grad_norm': '1.364', 'learning_rate': '3.373e-05', 'epoch': '0.6229'}\n",
            "{'loss': '1.215', 'grad_norm': '1.769', 'learning_rate': '2.711e-05', 'epoch': '0.6674'}\n",
            "{'loss': '1.242', 'grad_norm': '1.217', 'learning_rate': '2.096e-05', 'epoch': '0.7119'}\n",
            "{'loss': '1.235', 'grad_norm': '1.538', 'learning_rate': '1.542e-05', 'epoch': '0.7564'}\n",
            "{'loss': '1.217', 'grad_norm': '1.785', 'learning_rate': '1.059e-05', 'epoch': '0.8009'}\n",
            "{'loss': '1.389', 'grad_norm': '3.501', 'learning_rate': '6.579e-06', 'epoch': '0.8454'}\n",
            "{'loss': '1.214', 'grad_norm': '1.95', 'learning_rate': '3.469e-06', 'epoch': '0.8899'}\n",
            " 89% 200/225 [11:33<01:22,  3.29s/it][INFO|trainer.py:4438] 2026-02-13 08:56:15,093 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4440] 2026-02-13 08:56:15,094 >>   Num examples = 100\n",
            "[INFO|trainer.py:4443] 2026-02-13 08:56:15,094 >>   Batch size = 1\n",
            "\n",
            "  0% 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/100 [00:00<00:13,  7.35it/s]\u001b[A\n",
            "  3% 3/100 [00:00<00:17,  5.46it/s]\u001b[A\n",
            "  4% 4/100 [00:00<00:23,  4.07it/s]\u001b[A\n",
            "  5% 5/100 [00:01<00:27,  3.40it/s]\u001b[A\n",
            "  6% 6/100 [00:01<00:29,  3.21it/s]\u001b[A\n",
            "  7% 7/100 [00:01<00:29,  3.11it/s]\u001b[A\n",
            "  8% 8/100 [00:02<00:29,  3.14it/s]\u001b[A\n",
            "  9% 9/100 [00:02<00:27,  3.26it/s]\u001b[A\n",
            " 10% 10/100 [00:02<00:30,  2.98it/s]\u001b[A\n",
            " 11% 11/100 [00:03<00:27,  3.28it/s]\u001b[A\n",
            " 12% 12/100 [00:03<00:25,  3.41it/s]\u001b[A\n",
            " 13% 13/100 [00:03<00:24,  3.60it/s]\u001b[A\n",
            " 14% 14/100 [00:03<00:22,  3.76it/s]\u001b[A\n",
            " 15% 15/100 [00:04<00:24,  3.44it/s]\u001b[A\n",
            " 16% 16/100 [00:04<00:22,  3.67it/s]\u001b[A\n",
            " 17% 17/100 [00:04<00:23,  3.55it/s]\u001b[A\n",
            " 18% 18/100 [00:05<00:22,  3.64it/s]\u001b[A\n",
            " 19% 19/100 [00:05<00:23,  3.44it/s]\u001b[A\n",
            " 20% 20/100 [00:05<00:21,  3.65it/s]\u001b[A\n",
            " 21% 21/100 [00:05<00:22,  3.47it/s]\u001b[A\n",
            " 22% 22/100 [00:06<00:21,  3.70it/s]\u001b[A\n",
            " 23% 23/100 [00:06<00:21,  3.66it/s]\u001b[A\n",
            " 24% 24/100 [00:06<00:21,  3.56it/s]\u001b[A\n",
            " 25% 25/100 [00:06<00:20,  3.73it/s]\u001b[A\n",
            " 26% 26/100 [00:07<00:21,  3.51it/s]\u001b[A\n",
            " 27% 27/100 [00:07<00:19,  3.69it/s]\u001b[A\n",
            " 28% 28/100 [00:07<00:20,  3.48it/s]\u001b[A\n",
            " 29% 29/100 [00:08<00:19,  3.61it/s]\u001b[A\n",
            " 30% 30/100 [00:08<00:21,  3.32it/s]\u001b[A\n",
            " 31% 31/100 [00:08<00:20,  3.38it/s]\u001b[A\n",
            " 32% 32/100 [00:09<00:19,  3.47it/s]\u001b[A\n",
            " 33% 33/100 [00:09<00:18,  3.66it/s]\u001b[A\n",
            " 34% 34/100 [00:09<00:19,  3.36it/s]\u001b[A\n",
            " 35% 35/100 [00:09<00:19,  3.32it/s]\u001b[A\n",
            " 36% 36/100 [00:10<00:17,  3.58it/s]\u001b[A\n",
            " 37% 37/100 [00:10<00:17,  3.52it/s]\u001b[A\n",
            " 38% 38/100 [00:10<00:16,  3.72it/s]\u001b[A\n",
            " 39% 39/100 [00:10<00:16,  3.80it/s]\u001b[A\n",
            " 40% 40/100 [00:11<00:15,  3.97it/s]\u001b[A\n",
            " 41% 41/100 [00:11<00:14,  4.06it/s]\u001b[A\n",
            " 42% 42/100 [00:11<00:14,  4.05it/s]\u001b[A\n",
            " 43% 43/100 [00:11<00:14,  3.90it/s]\u001b[A\n",
            " 44% 44/100 [00:12<00:13,  4.02it/s]\u001b[A\n",
            " 45% 45/100 [00:12<00:13,  4.09it/s]\u001b[A\n",
            " 46% 46/100 [00:12<00:13,  4.04it/s]\u001b[A\n",
            " 47% 47/100 [00:13<00:14,  3.62it/s]\u001b[A\n",
            " 48% 48/100 [00:13<00:14,  3.48it/s]\u001b[A\n",
            " 49% 49/100 [00:13<00:16,  3.13it/s]\u001b[A\n",
            " 50% 50/100 [00:13<00:15,  3.29it/s]\u001b[A\n",
            " 51% 51/100 [00:14<00:14,  3.34it/s]\u001b[A\n",
            " 52% 52/100 [00:14<00:14,  3.38it/s]\u001b[A\n",
            " 53% 53/100 [00:14<00:14,  3.18it/s]\u001b[A\n",
            " 54% 54/100 [00:15<00:13,  3.37it/s]\u001b[A\n",
            " 55% 55/100 [00:15<00:12,  3.53it/s]\u001b[A\n",
            " 56% 56/100 [00:15<00:11,  3.72it/s]\u001b[A\n",
            " 57% 57/100 [00:15<00:11,  3.90it/s]\u001b[A\n",
            " 58% 58/100 [00:16<00:10,  3.96it/s]\u001b[A\n",
            " 59% 59/100 [00:16<00:10,  3.96it/s]\u001b[A\n",
            " 60% 60/100 [00:16<00:11,  3.63it/s]\u001b[A\n",
            " 61% 61/100 [00:16<00:10,  3.59it/s]\u001b[A\n",
            " 62% 62/100 [00:17<00:10,  3.65it/s]\u001b[A\n",
            " 63% 63/100 [00:17<00:09,  3.85it/s]\u001b[A\n",
            " 64% 64/100 [00:17<00:09,  3.91it/s]\u001b[A\n",
            " 65% 65/100 [00:17<00:08,  3.95it/s]\u001b[A\n",
            " 66% 66/100 [00:18<00:08,  4.03it/s]\u001b[A\n",
            " 67% 67/100 [00:18<00:08,  3.97it/s]\u001b[A\n",
            " 68% 68/100 [00:18<00:07,  4.10it/s]\u001b[A\n",
            " 69% 69/100 [00:19<00:08,  3.79it/s]\u001b[A\n",
            " 70% 70/100 [00:19<00:07,  3.94it/s]\u001b[A\n",
            " 71% 71/100 [00:19<00:07,  3.71it/s]\u001b[A\n",
            " 72% 72/100 [00:19<00:07,  3.50it/s]\u001b[A\n",
            " 73% 73/100 [00:20<00:07,  3.71it/s]\u001b[A\n",
            " 74% 74/100 [00:20<00:06,  3.82it/s]\u001b[A\n",
            " 75% 75/100 [00:20<00:07,  3.45it/s]\u001b[A\n",
            " 76% 76/100 [00:20<00:06,  3.59it/s]\u001b[A\n",
            " 77% 77/100 [00:21<00:06,  3.81it/s]\u001b[A\n",
            " 78% 78/100 [00:21<00:05,  3.90it/s]\u001b[A\n",
            " 79% 79/100 [00:21<00:05,  4.01it/s]\u001b[A\n",
            " 80% 80/100 [00:21<00:04,  4.10it/s]\u001b[A\n",
            " 81% 81/100 [00:22<00:04,  3.80it/s]\u001b[A\n",
            " 82% 82/100 [00:22<00:04,  3.91it/s]\u001b[A\n",
            " 83% 83/100 [00:22<00:04,  4.03it/s]\u001b[A\n",
            " 84% 84/100 [00:23<00:04,  3.55it/s]\u001b[A\n",
            " 85% 85/100 [00:23<00:04,  3.26it/s]\u001b[A\n",
            " 86% 86/100 [00:23<00:04,  3.49it/s]\u001b[A\n",
            " 87% 87/100 [00:23<00:03,  3.43it/s]\u001b[A\n",
            " 88% 88/100 [00:24<00:03,  3.46it/s]\u001b[A\n",
            " 89% 89/100 [00:24<00:03,  3.65it/s]\u001b[A\n",
            " 90% 90/100 [00:24<00:02,  3.53it/s]\u001b[A\n",
            " 91% 91/100 [00:24<00:02,  3.72it/s]\u001b[A\n",
            " 92% 92/100 [00:25<00:02,  3.60it/s]\u001b[A\n",
            " 93% 93/100 [00:25<00:02,  3.30it/s]\u001b[A\n",
            " 94% 94/100 [00:25<00:01,  3.40it/s]\u001b[A\n",
            " 95% 95/100 [00:26<00:01,  3.37it/s]\u001b[A\n",
            " 96% 96/100 [00:26<00:01,  3.05it/s]\u001b[A\n",
            " 97% 97/100 [00:26<00:00,  3.14it/s]\u001b[A\n",
            " 98% 98/100 [00:27<00:00,  3.06it/s]\u001b[A\n",
            " 99% 99/100 [00:27<00:00,  3.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': '1.269', 'eval_runtime': '28.12', 'eval_samples_per_second': '3.557', 'eval_steps_per_second': '3.557', 'epoch': '0.8899'}\n",
            " 89% 200/225 [12:01<01:22,  3.29s/it]\n",
            "100% 100/100 [00:27<00:00,  3.11it/s]\u001b[A\n",
            "{'loss': '1.205', 'grad_norm': '2.55', 'learning_rate': '1.323e-06', 'epoch': '0.9344'}\n",
            "{'loss': '1.156', 'grad_norm': '2.107', 'learning_rate': '1.868e-07', 'epoch': '0.9789'}\n",
            "100% 225/225 [13:25<00:00,  3.03s/it][INFO|trainer.py:4115] 2026-02-13 08:58:07,040 >> Saving model checkpoint to ./gemma_lora_sft_output/checkpoint-225\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:58:07,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:58:07,324 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:3327] 2026-02-13 08:58:07,437 >> chat template saved in ./gemma_lora_sft_output/checkpoint-225/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2181] 2026-02-13 08:58:07,438 >> tokenizer config file saved in ./gemma_lora_sft_output/checkpoint-225/tokenizer_config.json\n",
            "[INFO|trainer.py:2657] 2026-02-13 08:58:08,133 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': '806.3', 'train_samples_per_second': '1.115', 'train_steps_per_second': '0.279', 'train_loss': '1.382', 'epoch': '1'}\n",
            "100% 225/225 [13:26<00:00,  3.58s/it]\n",
            "[INFO|trainer.py:4115] 2026-02-13 08:58:08,137 >> Saving model checkpoint to ./gemma_lora_sft_output\n",
            "[INFO|configuration_utils.py:667] 2026-02-13 08:58:08,375 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-1.1-2b-it/snapshots/d750f5eceb83e978c09e2b3597c2a8784e381022/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-13 08:58:08,376 >> Model config GemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"use_bidirectional_attention\": null,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:3327] 2026-02-13 08:58:08,471 >> chat template saved in ./gemma_lora_sft_output/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2181] 2026-02-13 08:58:08,473 >> tokenizer config file saved in ./gemma_lora_sft_output/tokenizer_config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =  1805105GF\n",
            "  train_loss               =      1.382\n",
            "  train_runtime            = 0:13:26.34\n",
            "  train_samples_per_second =      1.115\n",
            "  train_steps_per_second   =      0.279\n",
            "Figure saved at: ./gemma_lora_sft_output/training_loss.png\n",
            "Figure saved at: ./gemma_lora_sft_output/training_eval_loss.png\n",
            "[WARNING|2026-02-13 08:58:13] llamafactory.extras.ploting:149 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4438] 2026-02-13 08:58:13,999 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4440] 2026-02-13 08:58:13,999 >>   Num examples = 100\n",
            "[INFO|trainer.py:4443] 2026-02-13 08:58:13,999 >>   Batch size = 1\n",
            "100% 100/100 [00:27<00:00,  3.60it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =     1.2681\n",
            "  eval_runtime            = 0:00:28.07\n",
            "  eval_samples_per_second =      3.562\n",
            "  eval_steps_per_second   =      3.562\n",
            "[INFO|modelcard.py:266] 2026-02-13 08:58:42,074 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!python -m llamafactory.cli train train_gemma_qlora.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "532ff4577ea0406e94f4b25bd0674156",
            "7dcfdcaf84db4ae0b3477ca64ea5dfe4",
            "1a3358b913764125b34ebf8ba915e2bc",
            "c6b03e5ab74a4448848f60e373fe48b2",
            "baa4c30402fc4305b6a86c31bd03791b",
            "586fbe194c054a2bb361b681fa5e1d6c",
            "449fcead96fe4cd2996cd93e2e3b5304",
            "04378942cd674a32ba3e7cbe714b9dfe",
            "998c660ed43547edbf9205afef2a8cc3",
            "bea2195f5634414baa797f5d84e06ba4",
            "de2bacd061a3406095d20e5f4bd407b9"
          ]
        },
        "id": "_xvhu4atJyQE",
        "outputId": "585d5958-0c1f-4d4e-a6a7-46c016eb110d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "532ff4577ea0406e94f4b25bd0674156",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is AI?\n",
            "\n",
            "Artificial intelligence (AI) is a broad field of computer science that encompasses the development of computer systems that can perform tasks that typically require human intelligence, such as problem-solving, decision-making, and natural language processing. AI systems are designed to learn and improve over time, and they can be used to automate tasks, make predictions, and generate creative content.\n",
            "\n",
            "AI has a wide range of applications, including healthcare, finance, transportation, and entertainment. It is also used in military and law enforcement applications.\n",
            "\n",
            "What are the different types of AI?\n",
            "\n",
            "There are many different types of AI, each with its own strengths and weaknesses. Some of the most common types of AI include:\n",
            "\n",
            "1. Machine learning: This type of AI uses algorithms to learn from data and make predictions or decisions. It is often used in areas such as image recognition, natural language processing, and self-driving cars.\n",
            "\n",
            "2. Deep learning: This type of AI uses artificial neural networks to learn from\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"google/gemma-1.1-2b-it\"\n",
        "adapter = \"/content/LLaMA-Factory/gemma_lora_sft_output\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(base, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "\n",
        "prompt = \"What is AI\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIv8FIMBPUQg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04378942cd674a32ba3e7cbe714b9dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a3358b913764125b34ebf8ba915e2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04378942cd674a32ba3e7cbe714b9dfe",
            "max": 164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_998c660ed43547edbf9205afef2a8cc3",
            "value": 164
          }
        },
        "449fcead96fe4cd2996cd93e2e3b5304": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532ff4577ea0406e94f4b25bd0674156": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dcfdcaf84db4ae0b3477ca64ea5dfe4",
              "IPY_MODEL_1a3358b913764125b34ebf8ba915e2bc",
              "IPY_MODEL_c6b03e5ab74a4448848f60e373fe48b2"
            ],
            "layout": "IPY_MODEL_baa4c30402fc4305b6a86c31bd03791b"
          }
        },
        "586fbe194c054a2bb361b681fa5e1d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dcfdcaf84db4ae0b3477ca64ea5dfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_586fbe194c054a2bb361b681fa5e1d6c",
            "placeholder": "",
            "style": "IPY_MODEL_449fcead96fe4cd2996cd93e2e3b5304",
            "value": "Loadingweights:100%"
          }
        },
        "998c660ed43547edbf9205afef2a8cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baa4c30402fc4305b6a86c31bd03791b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bea2195f5634414baa797f5d84e06ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b03e5ab74a4448848f60e373fe48b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bea2195f5634414baa797f5d84e06ba4",
            "placeholder": "",
            "style": "IPY_MODEL_de2bacd061a3406095d20e5f4bd407b9",
            "value": "164/164[00:12&lt;00:00,11.72it/s,Materializingparam=model.norm.weight]"
          }
        },
        "de2bacd061a3406095d20e5f4bd407b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
