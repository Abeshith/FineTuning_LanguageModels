# Fine-Tuning Language Models

## Overview
This repository provides an in-depth exploration of fine-tuning state-of-the-art language models using modern frameworks and techniques. It covers fine-tuning models like LLaMA, Mistral, and Gemma using efficient libraries like `transformers` and `unsloth`. The goal is to enhance model adaptability for specific tasks while optimizing computational efficiency.

![Fine-Tuning Language Models](https://github.com/Abeshith/FineTuning_LanguageModels/blob/main/banner.png)

## Technologies Used
- `Transformers`:   Utilized for loading, training, and fine-tuning transformer-based models.
- `UnsLoth`:   A lightweight and efficient framework for accelerating fine-tuning.
- `Hugging Face Hub`:  For accessing and sharing pre-trained models.
- `PyTorch`:   Backend framework for model training and optimization.

## Models And Functions:
- Various Models Used in Finetuning includes  `Mistral`,`Gemma`,`Llaam` etc.,
- Functions `AutoModelForCasualLm`, `AutoTokenizer`, `FastLanguageModel`, `Dpotrainer`, `PpotTrainer` and much more..,

## Getting Started
1. Clone the repository:
   ```bash
   git clone https://github.com/Abeshith/FineTuning_LanguageModels.git
